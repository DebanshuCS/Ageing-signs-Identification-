# Ageing-signs-detection-

## INTRODUCTION
The project was to develop a model to classify and localize different signs of ageing such as puffy eyes, wrinkles, dark spots, etc on the face. This project required knowledge of various domains and the use of image processing techniques to process the images as per requirements. The model was developed using Machine Learning, Artificial Intelligence and Deep Learning and hence we were able to detect dark spots, puffy eyes and wrinkles on images in order to understand ageing signs.


## MODEL SPECIFICATIONS
The model we developed is a classification model as the output columns are only 3.
The various packages used in building this model includes os, open,cv, numpy, keras, tensorflow and matplotlib.
The different types of flies used in this project are Json, csv and hv5.
A number of algorithms have been used for the development of this model. These various algorithms include Normalization, Image Augmentation, Image Preprocessing, Convolutional Neural Network and Grayscale.
Many functions have also been used in this project so as to get the desired output. The important of those functions are Max Pooling, Flatten, Dropout, Sequential, Adam Optimizer, Loss Function: Categorical Cross Entropy.
This model will work on any platform but we used Jupyter Notebook for the development of this model. All the packages used in this model are available in Jupyter Notebook as they are in-built in the program.

**I used CodeOcean to have reproducible run of this project**

## COURSE OF ACTION
The development of this model required lots of information and knowledge about image processing and also about training and testing of the model. Hence the knowledge acquired from the Artificial Intelligence course proved to be very useful during the course of this entire project. Machine Learning knowledge also played a great role in the successful running of this model.
First, a roadmap of the plan of action was designed. Then datasets were found for the experimentation of the model. Several datasets were downloaded from the internet and many other pictures were downloaded manually from the internet, and all the images were combined together to form a huge dataset, with which the model could be trained and tested. Later the code was written and corrected of all errors. The model was kept to be trained, this long process took several hours, and after testing the model, it was finally ready.
